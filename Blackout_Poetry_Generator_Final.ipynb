{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934b9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rishiheggawadi/anaconda3/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/rishiheggawadi/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /Users/rishiheggawadi/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rishiheggawadi/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /Users/rishiheggawadi/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cd42cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rishiheggawadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rishiheggawadi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rishiheggawadi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import messagebox, simpledialog\n",
    "\n",
    "\n",
    "def blackout(text):\n",
    "    global blackout_text\n",
    "    blackout_text = \"\"\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        in_range = False\n",
    "        for (start, end) in result_text_index:\n",
    "            if i >= start and i <= end:\n",
    "                in_range = True\n",
    "                break\n",
    "        if in_range:\n",
    "            blackout_text += char\n",
    "\n",
    "        elif char == \"\\n\":\n",
    "            blackout_text += \"\\n\"\n",
    "        elif char == \" \":\n",
    "            blackout_text += \" \"\n",
    "        else:\n",
    "            blackout_text += \"â–€\"\n",
    "            \n",
    "            \n",
    "def search_wiki(user_input):\n",
    "        global text, message, ambig\n",
    "        ambig = False\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                ambig = False\n",
    "                message = \"\"\n",
    "                text = wiki_pull(user_input)\n",
    "                if text == \"\":\n",
    "                    text = wiki_pull_backup(user_input)\n",
    "                    message = f\"Your search terms weren't a perfect match. Here's the closest article we found: {closest_match}\\n\"\n",
    "                return text, ambig\n",
    "                    \n",
    "            except Exception as e:\n",
    "#                 print(f\"An error occurred: {e}\")\n",
    "                text = wiki_pull_backup(user_input)\n",
    "                if \"may refer to\" not in text:\n",
    "                    return text, ambig\n",
    "                    \n",
    "                ambig = True\n",
    "                return None, ambig\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "#             print(f\"An error occurred: {e}\")\n",
    "            message = f\"\\nYour search terms didn't match any results! Please try again\"\n",
    "            update_poem()\n",
    "\n",
    "\n",
    "\n",
    "def wiki_pull(user_input):\n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": user_input}\n",
    "\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    # Check if page is a may-refer-to disambiguation page\n",
    "    if 'may refer to' in page_content or 'disambiguation' in page_content:\n",
    "        message = f\"\\nThe page for {user_input} is a disambiguation page. Please try again with a more specific search term.\"\n",
    "        raise ValueError(f\"The page for {user_input} is a disambiguation page.\")\n",
    "        \n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wiki_pull_backup(user_topic):\n",
    "    \n",
    "    wiki_api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # using search action instead of query\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": user_topic,\n",
    "        \"srprop\": \"\",\n",
    "        \"utf8\": \"\"}\n",
    "\n",
    "    # making request\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # selecting the page with closest match--IGNORING DISAMBIGUATION PAGES\n",
    "    global closest_match\n",
    "    for result in response_json[\"query\"][\"search\"]:\n",
    "        if \"disambiguation\" not in result[\"title\"].lower(): # check if snippet contains \"disambiguation\"\n",
    "            closest_match = result[\"title\"]\n",
    "            break\n",
    "\n",
    "    # using the selected title for content request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"exintro\": \"\", # get intro section only\n",
    "        \"explaintext\": \"\", # plain text\n",
    "        \"titles\": closest_match}\n",
    "\n",
    "    # making request for content\n",
    "    response = requests.get(wiki_api_url, params=params)\n",
    "    response_json = response.json()\n",
    "\n",
    "    # have to get page_id so we can index the json dictionary for the extract\n",
    "    page_id = list(response_json[\"query\"][\"pages\"].keys())[0]\n",
    "    page_content = response_json[\"query\"][\"pages\"][page_id][\"extract\"]\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the parts of speech that are allowed in the output\n",
    "allowed_pos = [\"JJ\", \"RB\", \"VB\", \"NN\", \"IN\"]\n",
    "\n",
    "\n",
    "result_tuples = []\n",
    "chosen = []\n",
    "\n",
    "# pops is used to limit the number of loops\n",
    "max_pops = 25\n",
    "global pops\n",
    "pops = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text2tokens(text):\n",
    "    global tokens, text_copy\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # to preserve a version of the original text\n",
    "    text_copy = text\n",
    "        \n",
    "    # attaching the index from the tokenized text AND the original text to each word (both are later added to result_tuple)\n",
    "    global both_indices\n",
    "    both_indices = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        text_index = text_copy.find(token) # searches for substring in string\n",
    "        text_index_end = text_index + (len(token)-1)\n",
    "        text_copy = text_copy.replace(token, \"x\" * len(token), 1) # replace the current token with all x's\n",
    "        both_indices.append((token, i, text_index, text_index_end))\n",
    "\n",
    "\n",
    "\n",
    "def get_word():\n",
    "    global choice, new_word, new_word_index, new_word_pos\n",
    "    choice = random.choice(range(len(tokens)))\n",
    "    chosen.append(choice)\n",
    "    new_word = tokens[choice]\n",
    "    new_word_index = choice\n",
    "    new_word_pos = pos_tag([new_word])[0][1][:2] # syntax: pos_tag generates a list of a tuple. This grabs the tuple, then grabs the 2nd element which is the POS as a string, then grabs the first 2 letters.\n",
    "    \n",
    "    \n",
    "    \n",
    "def sequence_check():\n",
    "    \"\"\"Checks to see if there is an sequence like 'adjective then noun' around the chosen word. \n",
    "    If so, then it adds both words to the results list.\"\"\"\n",
    "    \n",
    "    global new_word, new_word_index, new_word_pos\n",
    "    if choice > 0 and choice < len(tokens) - 1: # avoids list index out of range errors\n",
    "        prior_word = tokens[choice-1]\n",
    "        prior_word_index = choice-1\n",
    "        prior_word_pos = pos_tag([prior_word])[0][1][:2]\n",
    "\n",
    "        next_word = tokens[choice+1]\n",
    "        next_word_index = choice+1\n",
    "        next_word_pos = pos_tag([next_word])[0][1][:2]\n",
    "    \n",
    "    # adj-noun-verb\n",
    "        if prior_word_pos == \"JJ\" and new_word_pos == \"NN\" and next_word_pos ==\"VB\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "#             print(\"adj-noun-verb worked :)\")\n",
    "    \n",
    "    # adj-noun\n",
    "        elif prior_word_pos == \"JJ\" and new_word_pos == \"NN\":\n",
    "            new_word = prior_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()\n",
    "#             print(\"adj-noun worked :)\")\n",
    "\n",
    "        elif new_word_pos == \"JJ\" and next_word_pos == \"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "#             print(\"adj-noun worked :)\")\n",
    "     #modal-verb\n",
    "        elif prior_word_pos == \"MD\" and new_word_pos ==\"VB\":\n",
    "            new_word = next_word\n",
    "            new_word_index = prior_word_index\n",
    "            new_word_pos = prior_word_pos\n",
    "            check_and_insert()         \n",
    "        \n",
    "    #Determiner and noun\n",
    "        elif new_word_pos == \"DT\" and next_word_pos ==\"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "                \n",
    "    #Predeterminer and noun\n",
    "        elif new_word_pos == \"PDT\" and next_word_pos ==\"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "    #Preposition and noun\n",
    "        elif new_word_pos == \"IN\" and next_word_pos ==\"NN\":\n",
    "            new_word = next_word\n",
    "            new_word_index = next_word_index\n",
    "            new_word_pos = next_word_pos\n",
    "            check_and_insert()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def check_and_insert():\n",
    "    # Criteria:\n",
    "\n",
    "    # regular expression that matches only word characters. Prevents stuff like \"]\" which is a noun for some reason.\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    if pattern.match(new_word):\n",
    "    \n",
    "    \n",
    "    \n",
    "        if new_word_pos in allowed_pos:\n",
    "\n",
    "            # find the index to insert the new word at.\n",
    "            index_to_insert = len(result_tuples)\n",
    "            for i, (word, index, pos, text_index, text_index_end) in enumerate(result_tuples): # this uses tuple unpacking and the enumerate function\n",
    "                if new_word_index < index:\n",
    "                    index_to_insert = i\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "            # check if the new_word_pos is different from the POS of the words before and after the insertion point. checks 2 things:\n",
    "                # 1. does the word before the insertion point have a different POS, or is the new word gonna be the first word in the list.\n",
    "                # 2. does the word after the insertion point have a different POS, or is the new word gonna be the last word in the list.\n",
    "            if (index_to_insert == 0 or result_tuples[index_to_insert-1][2] != new_word_pos) and (index_to_insert == len(result_tuples) or result_tuples[index_to_insert][2] != new_word_pos):\n",
    "                \n",
    "                # getting index from original text\n",
    "                for (token, token_index, text_index, text_index_end) in both_indices:\n",
    "                    if new_word_index == token_index:\n",
    "                        new_text_index = text_index\n",
    "                        new_text_index_end = text_index_end\n",
    "                \n",
    "                # insert the new word's tuple into the result_tuples list\n",
    "                result_tuples.insert(index_to_insert, (new_word, new_word_index, new_word_pos, new_text_index, new_text_index_end))\n",
    "\n",
    "\n",
    "        if len(result_tuples) == random_length:\n",
    "\n",
    "\n",
    "            # force the first word to not be a verb\n",
    "            first_word_pos = result_tuples[0][2][:2]\n",
    "            if first_word_pos == \"VB\":\n",
    "                result_tuples.pop()\n",
    "                global pops\n",
    "                pops +=1\n",
    "    \n",
    "\n",
    "\n",
    "def display_results():\n",
    "    global result_words, result_index, result_pos, result_text_index\n",
    "    result_words = []\n",
    "    result_index = [] # result_index and result_pos lists are for easy reference to ensure the criteria worked\n",
    "    result_pos = []\n",
    "    result_text_index = []\n",
    "    for (word, index, pos, text_index, text_index_end) in result_tuples:\n",
    "        result_words.append(word)\n",
    "        result_index.append(index)\n",
    "        result_pos.append(pos)\n",
    "        result_text_index.append((text_index, text_index_end))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "while_counter = 0\n",
    "def update_poem():\n",
    "    user_input = search_entry.get()\n",
    "\n",
    "    text, ambig = search_wiki(user_input)\n",
    "    if ambig:\n",
    "        poem = f\"\\nThe page for {user_input} is a disambiguation page. Please try again with a more specific search term.\"\n",
    "        poemtext.insert(INSERT,poem)\n",
    "        poemtext.config(state = DISABLED)\n",
    "        scrolly = Scrollbar(poemtext)\n",
    "        poemtext.pack(anchor= 'center', expand = True)\n",
    "        return None\n",
    "    \n",
    "    text2tokens(text)\n",
    "    \n",
    "    global random_length, tokens\n",
    "    random_length = random.choice(range(7,round(len(tokens)/3)))\n",
    "    \n",
    "    while_counter = 0\n",
    "    while len(result_tuples) < random_length and pops < 25 and while_counter < 150:\n",
    "        get_word()\n",
    "        check_and_insert()\n",
    "        sequence_check()\n",
    "        \n",
    "        while_counter +=1\n",
    "    \n",
    "\n",
    "    display_results()\n",
    "    blackout(text)\n",
    "    \n",
    "    global result_string\n",
    "    delimiter = \" \"\n",
    "    result_string = delimiter.join(result_words)\n",
    "    \n",
    "    poem = f\"{message}\\n\\n{text}\"\n",
    "#     poem = f\"{message}\\n{len(result_words)}\\n{result_string}\\n\\n{text}\\n\\n{blackout_text}\\n\"\n",
    "    \n",
    "    poemtext.insert(INSERT,poem)\n",
    "    poemtext.config(state = DISABLED)\n",
    "    scrolly = Scrollbar(poemtext)\n",
    "    poemtext.pack(anchor= 'center', expand = True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def refresh():\n",
    "    global poemtext\n",
    "    user_input = search_entry.get()\n",
    "    \n",
    "    \n",
    "    pops = 0\n",
    "    \n",
    "    poemtext.destroy()\n",
    "    poemtext = Text(root, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 100, height = 100, font=(\"Verdana\", 16))  \n",
    "\n",
    "    if ambig:\n",
    "        poem = f\"\\nThe page for {user_input} is a disambiguation page. Please try again with a more specific search term.\"\n",
    "        poemtext.insert(INSERT,poem)\n",
    "        poemtext.config(state = DISABLED)\n",
    "        scrolly = Scrollbar(poemtext)\n",
    "        poemtext.pack(anchor= 'center', expand = True)\n",
    "#         return None\n",
    "    \n",
    "    \n",
    "    global result_tuples\n",
    "    result_tuples = []\n",
    "    \n",
    "    global random_length, tokens\n",
    "    random_length = random.choice(range(7,round(len(tokens)/3)))\n",
    "    \n",
    "    while_counter = 0\n",
    "    while len(result_tuples) < random_length and pops < 25 and while_counter < 150:\n",
    "        get_word()\n",
    "        check_and_insert()\n",
    "        sequence_check()\n",
    "        \n",
    "        while_counter +=1\n",
    "   \n",
    "    \n",
    "    display_results()\n",
    "    blackout(text)\n",
    "    \n",
    "    global result_string\n",
    "    delimiter = \" \"\n",
    "    result_string = delimiter.join(result_words)\n",
    "    \n",
    "    poem = f\"{message}\\n\\n{text}\"\n",
    "    \n",
    "    poemtext.insert(INSERT,poem)\n",
    "    poemtext.config(state = DISABLED)\n",
    "    scrolly = Scrollbar(poemtext)\n",
    "    poemtext.pack(anchor= 'center', expand = True)\n",
    "    refresh_button.config(state=\"normal\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def blackout_button():\n",
    "    global poemtext\n",
    "    poemtext.destroy()\n",
    "    poemtext = Text(root, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 100, height = 100, font=(\"Verdana\", 16))  \n",
    "    \n",
    "    poem = f\"{message}\\n\\n{blackout_text}\"    \n",
    "    \n",
    "    poemtext.insert(INSERT,poem)\n",
    "    poemtext.config(state = DISABLED)\n",
    "    scrolly = Scrollbar(poemtext)\n",
    "    poemtext.pack(anchor= 'center', expand = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#tkinter setup\n",
    "root = tk.Tk()\n",
    "root.geometry(\"800x650\")\n",
    "root.title('Blackout Poetry Generator')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#search section\n",
    "search_frame = tk.Frame(root)\n",
    "search_frame.pack(pady=10)\n",
    "\n",
    "search_label = tk.Label(search_frame, text='Enter a search term: ')\n",
    "search_label.pack(side=tk.LEFT)\n",
    "\n",
    "search_entry = tk.Entry(search_frame, width=30)\n",
    "search_entry.pack(side=tk.LEFT)\n",
    "\n",
    "search_button = tk.Button(search_frame, text='search', command=update_poem)\n",
    "search_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "#poem section\n",
    "poem_frame = tk.Frame(root)\n",
    "poem_frame.pack(pady=10)\n",
    "\n",
    "poemtext = Text(root, wrap = \"word\", spacing1 = 1, spacing2 = 2, spacing3 = 1, width = 100, height = 100, font=(\"Verdana\", 16))  \n",
    "\n",
    "# button to trigger the blackout function\n",
    "blackout_button_tk = Button(poem_frame, text=\"Blackout\", command=blackout_button)\n",
    "blackout_button_tk.pack(pady=10)\n",
    "\n",
    "refresh_button = tk.Button(poem_frame, text='Refresh poem', command=refresh)\n",
    "refresh_button.pack()\n",
    "\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d65811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
